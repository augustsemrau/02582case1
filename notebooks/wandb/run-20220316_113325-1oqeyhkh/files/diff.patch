diff --git a/notebooks/model_research.ipynb b/notebooks/model_research.ipynb
index 99c7a3e..13add76 100644
--- a/notebooks/model_research.ipynb
+++ b/notebooks/model_research.ipynb
@@ -20,10 +20,11 @@
     "import warnings\n",
     "warnings.filterwarnings('ignore')\n",
     "\n",
-    "from sklearn import linear_model\n",
     "import numpy as np\n",
     "import matplotlib.pyplot as plt\n",
     "\n",
+    "from sklearn.linear_model import ElasticNet\n",
+    "from sklearn.ensemble import RandomForestRegressor\n",
     "from xgboost import XGBRegressor"
    ]
   },
@@ -499,7 +500,18 @@
     {
      "data": {
       "text/plain": [
-       "(39446,)"
+       "0        0.408451\n",
+       "1        0.189189\n",
+       "2        0.570423\n",
+       "3        0.333333\n",
+       "4        0.204301\n",
+       "           ...   \n",
+       "39444    0.847222\n",
+       "39445    0.871795\n",
+       "39446    0.857143\n",
+       "39447    0.682796\n",
+       "39448    0.820513\n",
+       "Name: LoadFactor, Length: 39446, dtype: float64"
       ]
      },
      "execution_count": 5,
@@ -509,7 +521,7 @@
    ],
    "source": [
     "### Print shape of target\n",
-    "y.shape"
+    "y"
    ]
   },
   {
@@ -1388,7 +1400,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 19,
+   "execution_count": 9,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -1396,7 +1408,7 @@
     "\n",
     "### Make train/val set *0.8 and test *0.2\n",
     "def split_model_test(X, y, seed=0, shuffle=False):\n",
-    "    X_model, X_test, y_model, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, shuffle=shuffle);\n",
+    "    X_model, X_test, y_model, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, shuffle=shuffle)\n",
     "    return X_model, X_test, y_model, y_test\n",
     "\n",
     "def split_train_val(X_m, y_m, seed=0, shuffle=False):\n",
@@ -1430,13 +1442,12 @@
     "\n",
     "    passengers_true = loadfactor_true * seatcapacity\n",
     "    passengers_forecasted = loadfactor_forecasted * seatcapacity\n",
-    "    # eps = np.finfo(float).eps\n",
     "    \n",
     "    abs_deviation_per_flight = np.abs((passengers_true-passengers_forecasted) / passengers_true)\n",
     "    abs_deviation_per_flight[abs_deviation_per_flight >= 10000] = 100\n",
     "\n",
-    "    mean_forecast_acc = np.mean(100 - abs_deviation_per_flight)\n",
-    "    print(f'Mean forecast accuracy = {mean_forecast_acc}')\n",
+    "    mean_forecast_acc = np.mean(1 - abs_deviation_per_flight*1)*100\n",
+    "    # print(f'Mean forecast accuracy = {mean_forecast_acc}')\n",
     "    return mean_forecast_acc"
    ]
   },
@@ -1444,7 +1455,7 @@
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "## Define nomralizer for training on **SeatCapacity**"
+    "## Define normalizer for training on **SeatCapacity**"
    ]
   },
   {
@@ -1453,13 +1464,13 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "def normalize_seatcapacity_train(X_train):\n",
+    "def normalize_seatcapacity_fit(X_train):\n",
     "    scaler = MinMaxScaler()\n",
     "    scaler.fit(X_train.SeatCapacity.values.reshape(-1, 1))\n",
     "    X_train.SeatCapacity = scaler.transform(X_train.SeatCapacity.values.reshape(-1, 1))\n",
     "    return X_train, scaler\n",
     "\n",
-    "def normalize_seatcapacity_val(X_val, scaler):\n",
+    "def normalize_seatcapacity(X_val, scaler):\n",
     "    X_val.SeatCapacity = scaler.transform(X_val.SeatCapacity.values.reshape(-1, 1))\n",
     "    return X_val"
    ]
@@ -1478,113 +1489,84 @@
    "outputs": [],
    "source": [
     "### Make function for fitting and validating model\n",
-    "def train_validate_model(X_train, X_val, y_train, y_val, model):\n",
+    "def fit_evaluate_model(X_tr_m, X_v_te, y_tr_m, y_v_te, model):\n",
     "    \n",
     "    ## Remove original seatcapacity\n",
-    "    X_train, X_val, X_train_SCO, X_val_SCO = seperate_SCO(X_train_model=X_train, X_val_test=X_val)\n",
-    "\n",
-    "    ## Normalize seatcapacity\n",
-    "    X_train, fitted_scaler = normalize_seatcapacity_train(X_train=X_train)\n",
+    "    X_tr_m, X_v_te, X_tr_m_SCO, X_v_te_SCO = seperate_SCO(X_train_model=X_tr_m, X_val_test=X_v_te)\n",
     "\n",
+    "    # ## Normalize seatcapacity\n",
+    "    # X_tr_m, fitted_scaler = normalize_seatcapacity_fit(X_train=X_tr_m)\n",
     "    ## Fit model to the training data\n",
-    "    model.fit(X=X_train, y=y_train)\n",
+    "    model.fit(X=X_tr_m, y=y_tr_m)\n",
     "\n",
-    "    ## Normalize validation data SeatCapacity for predictions\n",
-    "    X_val = normalize_seatcapacity_val(X_val=X_val, scaler=fitted_scaler)\n",
-    "\n",
-    "    \n",
+    "    # ## Normalize validation data SeatCapacity for predictions\n",
+    "    # X_v_te = normalize_seatcapacity(X_val=X_v_te, scaler=fitted_scaler)\n",
     "    ## Make predictions\n",
-    "    val_pred = model.predict(X_val)\n",
+    "    pred = model.predict(X_v_te)\n",
     "\n",
     "    ## Compute forecasting accuracy\n",
-    "    val_acc = mean_forecast_accuracy(loadfactor_forecasted=val_pred, loadfactor_true=y_val.to_numpy(), seatcapacity=X_val_SCO.to_numpy())\n",
+    "    acc = mean_forecast_accuracy(loadfactor_forecasted=pred, loadfactor_true=y_v_te.to_numpy(), seatcapacity=X_v_te_SCO.to_numpy())\n",
     "\n",
-    "    return val_acc, model\n",
-    "\n",
-    "\n",
-    "### Make function for fitting model to all modeling data at validating on test set\n",
-    "def train_test_model(X_model, y_model, X_test, y_test, model):\n",
-    "    \n",
-    "    ## Remove original seatcapacity\n",
-    "    X_model, X_test, X_model_SCO, X_test_SCO = seperate_SCO(X_train_model=X_model, X_val_test=X_test)\n",
-    "\n",
-    "    ## Normalize seatcapacity\n",
-    "    X_model, fitted_scaler = normalize_seatcapacity_train(X_train=X_model)\n",
-    "\n",
-    "    ## Fit model to the training data\n",
-    "    model.fit(X=X_model, y=y_model)\n",
-    "\n",
-    "    ## Normalize validation data SeatCapacity for predictions\n",
-    "    X_test = normalize_seatcapacity_val(X_val=X_test, scaler=fitted_scaler)\n",
-    "\n",
-    "    \n",
-    "    ## Make predictions\n",
-    "    test_pred = model.predict(X_test)\n",
-    "\n",
-    "    ## Compute forecasting accuracy\n",
-    "    test_acc = mean_forecast_accuracy(loadfactor_forecasted=test_pred, loadfactor_true=y_test.to_numpy(), seatcapacity=X_test_SCO.to_numpy())\n",
-    "\n",
-    "    return test_acc, model"
+    "    return acc, model"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "## Linear models"
+    "# Compute EPE ground function"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 18,
+   "execution_count": 13,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Mean forecast accuracy = 54.34156780635255\n",
-      "Linear Regresion Forecast Accuracy: 54.34156780635255\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
-    "### Train-Validation run\n",
+    "# shuffle = True\n",
+    "# M = 100\n",
+    "# forecast_acc = []\n",
+    "\n",
+    "# for m in range(M):\n",
     "\n",
-    "X_model, X_test, y_model, y_test = split_model_test(X=X, y=y, seed=1, shuffle=False)\n",
-    "X_train, X_val, y_train, y_val = split_train_val(X_m=X_model, y_m=y_model, seed=1, shuffle=False)\n",
+    "#     ## Split data\n",
+    "#     seed = np.random.randint(10000)\n",
+    "#     X_model, X_test, y_model, y_test = split_model_test(X=X, y=y, seed=seed, shuffle=shuffle)\n",
+    "#     X_train, X_val, y_train, y_val = split_train_val(X_m=X_model, y_m=y_model, seed=seed, shuffle=shuffle)\n",
     "\n",
-    "### Linear Regression\n",
-    "linreg_model = linear_model.LinearRegression()\n",
-    "linreg_val_acc, linreg_trained_model = train_validate_model(X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val, model=linreg_model)\n",
+    "#     ## Train model on training data with different model parameters\n",
+    "#     #TODO MODEL TRAINING FUNCTION\n",
+    "#     val_accs, best_model = \n",
     "\n",
-    "print(f'Linear Regresion Forecast Accuracy: {linreg_val_acc}')"
+    "#     ## Evaluate best model on test data\n",
+    "#     test_acc, _ = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_test, y_tr_m=y_train, y_v_te=y_test, model=best_model)\n",
+    "#     forecast_acc.append(test_acc)\n"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Elastic Net"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 14,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Mean forecast accuracy = 96.03164764998851\n",
-      "Linear Regresion Forecast Accuracy: 96.03164764998851\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
-    "### Model-test run\n",
-    "\n",
-    "X_model, X_test, y_model, y_test = split_model_test(X=X, y=y, seed=0, shuffle=False)\n",
+    "def find_best_elastic_model(X_train, X_val, y_train, y_val):\n",
+    "    \n",
+    "    val_accs = {}\n",
+    "    alphas = np.linspace(0,20,20)\n",
+    "    for a in alphas[1:]:\n",
+    "        model = ElasticNet(alpha=a, l1_ratio=0.5)\n",
+    "        val_acc, model = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_val, y_tr_m=y_train, y_v_te=y_val, model=model)\n",
+    "        val_accs[a] = val_acc\n",
     "\n",
-    "### Linear Regression\n",
-    "linreg_model = linear_model.LinearRegression()\n",
-    "linreg_test_acc, linreg_fully_trained_model = train_test_model(X_model=X_model, y_model=y_model, X_test=X_test, y_test=y_test, model=linreg_model)\n",
     "\n",
-    "print(f'Linear Regresion Forecast Accuracy: {linreg_test_acc}')"
+    "    return val_accs"
    ]
   },
   {
@@ -1595,7 +1577,25 @@
     {
      "data": {
       "text/plain": [
-       "5471"
+       "{1.0526315789473684: -42.31303067921718,\n",
+       " 2.1052631578947367: -41.99731061421685,\n",
+       " 3.1578947368421053: -41.960908325347205,\n",
+       " 4.2105263157894735: -41.960908325347205,\n",
+       " 5.263157894736842: -41.960908325347205,\n",
+       " 6.315789473684211: -41.960908325347205,\n",
+       " 7.368421052631579: -41.960908325347205,\n",
+       " 8.421052631578947: -41.960908325347205,\n",
+       " 9.473684210526315: -41.960908325347205,\n",
+       " 10.526315789473683: -41.960908325347205,\n",
+       " 11.578947368421051: -41.960908325347205,\n",
+       " 12.631578947368421: -41.960908325347205,\n",
+       " 13.68421052631579: -41.960908325347205,\n",
+       " 14.736842105263158: -41.960908325347205,\n",
+       " 15.789473684210526: -41.960908325347205,\n",
+       " 16.842105263157894: -41.960908325347205,\n",
+       " 17.894736842105264: -41.960908325347205,\n",
+       " 18.94736842105263: -41.960908325347205,\n",
+       " 20.0: -41.960908325347205}"
       ]
      },
      "execution_count": 15,
@@ -1604,8 +1604,13 @@
     }
    ],
    "source": [
-    "def \n",
-    "np.random.randint(10000)"
+    "seed = 1\n",
+    "shuffle = True\n",
+    "X_model, X_test, y_model, y_test = split_model_test(X=X, y=y, seed=seed, shuffle=shuffle)\n",
+    "X_train, X_val, y_train, y_val = split_train_val(X_m=X_model, y_m=y_model, seed=seed, shuffle=shuffle)\n",
+    "accs = find_best_elastic_model(X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val)\n",
+    "\n",
+    "accs"
    ]
   },
   {
@@ -1617,111 +1622,191 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "alpha = 0.02040816326530612\n"
-     ]
-    },
-    {
-     "ename": "TypeError",
-     "evalue": "train_validate_model() got an unexpected keyword argument 'X_model'",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
-      "\u001b[0;32m<ipython-input-16-124136019c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'alpha = {i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mridge_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mridge_val_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mridge_trained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_validate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mridge_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mridge_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mridge_val_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;31mTypeError\u001b[0m: train_validate_model() got an unexpected keyword argument 'X_model'"
+      "Mean of test accuracies: -36.93078545910262\n",
+      "Std. of test accuracies: 38.302976932198796\n"
      ]
     }
    ],
    "source": [
-    "### Ridge Regression\n",
+    "shuffle = True\n",
+    "M = 100\n",
+    "forecast_acc = []\n",
+    "\n",
+    "for m in range(M):\n",
     "\n",
-    "ridge_acc = []\n",
-    "alphas = np.linspace(start=0, stop=1, num=50)[1:]\n",
-    "for i in alphas:\n",
-    "    print(f'alpha = {i}')\n",
-    "    ridge_model = linear_model.Ridge(alpha=float(i))\n",
-    "    ridge_val_acc, ridge_trained_model = train_validate_model(X_model=X_model, y_model=y_model, model=ridge_model)\n",
+    "    ## Split data\n",
+    "    seed = np.random.randint(10000)\n",
+    "    X_model, X_test, y_model, y_test = split_model_test(X=X, y=y, seed=seed, shuffle=shuffle)\n",
+    "    X_train, X_val, y_train, y_val = split_train_val(X_m=X_model, y_m=y_model, seed=seed, shuffle=shuffle)\n",
     "\n",
-    "    ridge_acc.append(ridge_val_acc)\n",
+    "    ## Train model on training data with different model parameters\n",
+    "    #TODO MODEL TRAINING FUNCTION\n",
+    "    # val_accs, best_model = \n",
+    "    model = ElasticNet(alpha=1, l1_ratio=0.5)\n",
     "\n",
-    "plt.plot(alphas, ridge_acc)"
+    "    ## Evaluate best model on test data\n",
+    "    test_acc, _ = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_test, y_tr_m=y_train, y_v_te=y_test, model=model)\n",
+    "    forecast_acc.append(test_acc)\n",
+    "\n",
+    "\n",
+    "print(f'Mean of test accuracies: {np.mean(forecast_acc)}\\nStd. of test accuracies: {np.var(forecast_acc)}')"
    ]
   },
   {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
-    "# Tree Models"
+    "# Random Forest"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "# Gradient Boosted Trees"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 17,
+   "metadata": {},
    "outputs": [
     {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Mean forecast accuracy = 99.35487740825978\n",
-      "XGBoost Regresion Forecast Accuracy on training data: 99.35487740825978\n"
-     ]
+     "data": {
+      "text/plain": [
+       "{1.0526315789473684: 19.58984204460955,\n",
+       " 2.1052631578947367: 21.058953759016994,\n",
+       " 3.1578947368421053: 22.283512332578965,\n",
+       " 4.2105263157894735: 23.958080410210307,\n",
+       " 5.263157894736842: 25.971673017382198,\n",
+       " 6.315789473684211: 24.767662321526526,\n",
+       " 7.368421052631579: 25.529127127637107,\n",
+       " 8.421052631578947: 28.142643562870017,\n",
+       " 9.473684210526315: 25.737688864517487,\n",
+       " 10.526315789473683: 25.056291342813825,\n",
+       " 11.578947368421051: 26.823367295982216,\n",
+       " 12.631578947368421: 27.352924724919692,\n",
+       " 13.68421052631579: 25.958576058437455,\n",
+       " 14.736842105263158: 27.496819283086943,\n",
+       " 15.789473684210526: 28.383813965072633,\n",
+       " 16.842105263157894: 26.273853005103508,\n",
+       " 17.894736842105264: 27.390566100573782,\n",
+       " 18.94736842105263: 26.66629320010499,\n",
+       " 20.0: 27.852899595919347}"
+      ]
+     },
+     "execution_count": 17,
+     "metadata": {},
+     "output_type": "execute_result"
     }
    ],
    "source": [
-    "X_model, X_test, y_model, y_test = split_model_test(X, y, seed=0)\n",
+    "def find_best_xgboost_model(X_train, X_val, y_train, y_val):\n",
+    "    \n",
+    "    val_accs = {}\n",
+    "    alphas = np.linspace(0,20,20)\n",
+    "    for a in alphas[1:]:\n",
+    "        model = XGBRegressor(max_depth=int(a))\n",
+    "        val_acc, model = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_val, y_tr_m=y_train, y_v_te=y_val, model=model)\n",
+    "        val_accs[a] = val_acc\n",
     "\n",
-    "### XGBoost Regression\n",
-    "xgb_model = XGBRegressor()\n",
-    "xgb_val_acc, xgb_trained_model = train_validate_model(X_model=X_model, y_model=y_model, model=xgb_model)\n",
     "\n",
-    "print(f'XGBoost Regresion Forecast Accuracy on training data: {xgb_val_acc}')"
+    "    return val_accs, max(val_accs, key=val_accs.get)\n",
+    "\n",
+    "seed = 1\n",
+    "shuffle = False\n",
+    "X_model, X_test, y_model, y_test = split_model_test(X=X, y=y, seed=seed, shuffle=shuffle)\n",
+    "X_train, X_val, y_train, y_val = split_train_val(X_m=X_model, y_m=y_model, seed=seed, shuffle=shuffle)\n",
+    "accs, max_acc = find_best_xgboost_model(X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val)\n",
+    "\n",
+    "accs"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 18,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Mean forecast accuracy = 99.36444654776558\n",
-      "XGBoost Regresion Forecast Accuracy on modeling data: 99.36444654776558\n"
+      "Mean of test accuracies: 33.98719329671349\n",
+      "Std. of test accuracies: 14.867881822567838\n"
      ]
     }
    ],
    "source": [
-    "X_model, X_test, y_model, y_test = split_model_test(X, y, seed=0)\n",
+    "shuffle = True\n",
+    "M = 10\n",
+    "forecast_acc = []\n",
+    "\n",
+    "for m in range(M):\n",
+    "\n",
+    "    ## Split data\n",
+    "    seed = np.random.randint(10000)\n",
+    "    X_model, X_test, y_model, y_test = split_model_test(X=X, y=y, seed=seed, shuffle=shuffle)\n",
+    "    X_train, X_val, y_train, y_val = split_train_val(X_m=X_model, y_m=y_model, seed=seed, shuffle=shuffle)\n",
+    "    # print(f'Shape X_model = {X_train.shape}')\n",
+    "\n",
+    "    ## Train model on training data with different model parameters\n",
+    "    #TODO MODEL TRAINING FUNCTION\n",
+    "    # val_accs, best_model = \n",
+    "    model = XGBRegressor()\n",
+    "\n",
+    "    ## Evaluate best model on test data\n",
+    "    test_acc, _ = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_test, y_tr_m=y_train, y_v_te=y_test, model=model)\n",
+    "    forecast_acc.append(test_acc)\n",
     "\n",
-    "### XGBoost Regression\n",
-    "xgb_model = XGBRegressor()\n",
-    "xgb_test_acc, xgb_fully_trained_model = train_test_model(X_model=X_model, y_model=y_model, X_test=X_test, y_test=y_test, model=xgb_model)\n",
     "\n",
-    "print(f'XGBoost Regresion Forecast Accuracy on modeling data: {xgb_test_acc}')"
+    "print(f'Mean of test accuracies: {np.mean(forecast_acc)}\\nStd. of test accuracies: {np.var(forecast_acc)}')"
    ]
   },
   {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
+   "cell_type": "markdown",
    "metadata": {},
-   "outputs": [],
-   "source": []
+   "source": [
+    "# Make WandB Sweeps to preliminarily find best ranges for parameters"
+   ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 19,
    "metadata": {},
-   "outputs": [],
-   "source": []
+   "outputs": [
+    {
+     "ename": "NameError",
+     "evalue": "name 'device' is not defined",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-19-43d7d1d975b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m wandb_config = {'seed': seed,\n\u001b[0;32m----> 3\u001b[0;31m                 \u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                 \u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
+     ]
+    }
+   ],
+   "source": [
+    "import wandb\n",
+    "wandb_config = {'learning_rate': learning_rate,\n",
+    "\n",
+    "                }\n",
+    "\n",
+    "## Initialize WandB for logging config and metrics\n",
+    "wandb.init(project='02582_case1', entity='tgml', config=wandb_config)\n",
+    "\n",
+    "## Plot and log event distribution\n",
+    "plot_event_dist(dataset=dataset_full, wandb_handler=wandb)\n",
+    "\n",
+    "wandb.log({'training_set_size': training_set_size, 'removed_node_pairs': removed_node_pairs, 'train_batch_size': train_batch_size, 'beta': model_beta})\n"
+   ]
   }
  ],
  "metadata": {
