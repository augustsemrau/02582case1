{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case One: Project Notebook\n",
    "By August and William"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c09e8debfb98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 1. Data Loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m## Load data and remove nan's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mreal_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mreal_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mreal_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/.csv'"
     ]
    }
   ],
   "source": [
    "### Imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Data Loading\n",
    "## Load data and remove nan's\n",
    "real_train = pd.read_csv('data/.csv', index_col=0)\n",
    "real_val = pd.read_csv('data/.csv', index_col=0)\n",
    "real_test = pd.read_csv('data/.csv', index_col=0)\n",
    "\n",
    "future = pd.read_csv('data/.csv', index_col=0)\n",
    "\n",
    "# Remove target from data\n",
    "y_train = real_train.LoadFactor\n",
    "real_train = real_train.loc[:, real_train.columns != 'LoadFactor']\n",
    "y_val = real_val.LoadFactor\n",
    "real_val = real_val.loc[:, real_val.columns != 'LoadFactor']\n",
    "\n",
    "## Make copy of **SeatCapacity** for computing forecast accuracy\n",
    "real_train['SeatCapacityOriginal'] = real_train.SeatCapacity\n",
    "real_val['SeatCapacityOriginal'] = real_val.SeatCapacity\n",
    "real_test['SeatCapacityOriginal'] = real_test.SeatCapacity\n",
    "# future['SeatCapacityOriginal'] = future.SeatCapacity\n",
    "\n",
    "X_train = real_train\n",
    "X_val = real_val\n",
    "\n",
    "\n",
    "# # 3. Data splitting\n",
    "# ## Split data into modeling data (will be training and validation) and test data\n",
    "\n",
    "# ### Make train/val set *0.8 and test *0.2\n",
    "# def split_model_test(X, y, seed=0, shuffle=False, stratify=False):\n",
    "#     if stratify:\n",
    "#         X_model, X_test, y_model, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, shuffle=shuffle, stratify=y)\n",
    "#     else:\n",
    "#         X_model, X_test, y_model, y_test = train_test_split(X, y, test_size=0.2, random_state=seed, shuffle=shuffle)\n",
    "#     return X_model, X_test, y_model, y_test\n",
    "\n",
    "# def split_train_val(X_m, y_m, seed=0, shuffle=False, stratify=False):\n",
    "#     if stratify:\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(X_m, y_m, test_size=0.25, random_state=seed, shuffle=shuffle, stratify=y_m)\n",
    "#     else:\n",
    "#         X_train, X_val, y_train, y_val = train_test_split(X_m, y_m, test_size=0.25, random_state=seed, shuffle=shuffle)\n",
    "#     return X_train, X_val, y_train, y_val\n",
    "\n",
    "def seperate_SCO(X_train_model, X_val_test):\n",
    "    X_train_model_SCO, X_val_test_SCO = X_train_model.SeatCapacityOriginal, X_val_test.SeatCapacityOriginal\n",
    "\n",
    "    X_train_model = X_train_model.loc[:, ~X_train_model.columns.isin(['SeatCapacityOriginal'])]\n",
    "    X_val_test = X_val_test.loc[:, ~X_val_test.columns.isin(['SeatCapacityOriginal'])]\n",
    "\n",
    "    return X_train_model, X_val_test, X_train_model_SCO, X_val_test_SCO\n",
    "\n",
    "\n",
    "# 4. Define validation setup for different models\n",
    "## Define forecast accuracy function\n",
    "def mean_forecast_accuracy(loadfactor_forecasted, loadfactor_true, seatcapacity):\n",
    "\n",
    "    passengers_true = loadfactor_true * seatcapacity\n",
    "    passengers_forecasted = loadfactor_forecasted * seatcapacity\n",
    "    \n",
    "    abs_deviation_per_flight = np.abs((passengers_true-passengers_forecasted) / passengers_true)\n",
    "    abs_deviation_per_flight[abs_deviation_per_flight >= 10000] = 100\n",
    "\n",
    "    mean_forecast_acc = np.mean(1 - abs_deviation_per_flight*1)*100\n",
    "    return mean_forecast_acc\n",
    "\n",
    "## Define normalizer for training on **SeatCapacity**\n",
    "def normalize_seatcapacity_fit(X_train):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train.SeatCapacity.values.reshape(-1, 1))\n",
    "    X_train.SeatCapacity = scaler.transform(X_train.SeatCapacity.values.reshape(-1, 1))\n",
    "    return X_train, scaler\n",
    "\n",
    "def normalize_seatcapacity(X_val, scaler):\n",
    "    X_val.SeatCapacity = scaler.transform(X_val.SeatCapacity.values.reshape(-1, 1))\n",
    "    return X_val\n",
    "\n",
    "## Functions for fitting+validating models, as well as testing models\n",
    "### Make function for fitting and validating model\n",
    "def fit_evaluate_model(X_tr_m, X_v_te, y_tr_m, y_v_te, model):\n",
    "    \n",
    "    ## Remove original seatcapacity\n",
    "    X_tr_m, X_v_te, X_tr_m_SCO, X_v_te_SCO = seperate_SCO(X_train_model=X_tr_m, X_val_test=X_v_te)\n",
    "\n",
    "    ## Normalize seatcapacity\n",
    "    X_tr_m, fitted_scaler = normalize_seatcapacity_fit(X_train=X_tr_m)\n",
    "    ## Fit model to the training data\n",
    "    model.fit(X=X_tr_m, y=y_tr_m)\n",
    "\n",
    "    ## Normalize validation data SeatCapacity for predictions\n",
    "    X_v_te = normalize_seatcapacity(X_val=X_v_te, scaler=fitted_scaler)\n",
    "    ## Make predictions\n",
    "    pred = model.predict(X_v_te)\n",
    "\n",
    "    ## Compute forecasting accuracy\n",
    "    acc = mean_forecast_accuracy(loadfactor_forecasted=pred, loadfactor_true=y_v_te.to_numpy(), seatcapacity=X_v_te_SCO.to_numpy())\n",
    "\n",
    "    return acc, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Find best hyperparameters using val set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\", # try grid or random\n",
    "    \"metric\": {\n",
    "      \"name\": \"accuracy\",\n",
    "      \"goal\": \"maximize\"   \n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"n_estimators\": {\n",
    "            \"values\": [400, 800, 1200, 1600, 2000]\n",
    "        },\n",
    "        \"max_depth\": {\n",
    "            \"values\": [5,7,9,11,13]\n",
    "            # \"distribution\": \"uniform\",\n",
    "            # \"min\": 0.00001,\n",
    "            # \"max\": 1\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.1, 0.15, 0.2]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='02582_case1_gb', entity='tgml')\n",
    "\n",
    "def train():\n",
    "    config_defaults = {\n",
    "    # 'bootstrap': True,\n",
    "    # 'criterion': 'mse',\n",
    "    # 'max_features': 'auto',\n",
    "    # 'max_leaf_nodes': None,\n",
    "    # 'min_impurity_decrease': 0.0,\n",
    "    # 'min_impurity_split': None,\n",
    "    # 'min_samples_leaf': 1,\n",
    "    # 'min_samples_split': 2,\n",
    "    # 'min_weight_fraction_leaf': 0.0,\n",
    "    # 'n_estimators': 10,\n",
    "    # 'n_jobs': 1,\n",
    "    # 'oob_score': False,\n",
    "    # 'random_state': 42,\n",
    "    # 'verbose': 0,\n",
    "    # 'warm_start': False,\n",
    "    \"seed\": 0,\n",
    "    \"shuffle\": True,\n",
    "    'max_depth': 3,\n",
    "    \"n_estimators\": 100,\n",
    "    \"learning_rate\": 0.1,\n",
    "    }\n",
    "\n",
    "    wandb.init(project='02582_case1_1', entity='tgml', config=config_defaults)  # defaults are over-ridden during the sweep\n",
    "    config = wandb.config\n",
    "\n",
    "    # fit model on train\n",
    "    model = GradientBoostingRegressor(n_estimators=config.n_estimators, \n",
    "                                        learning_rate=config.learning_rate,\n",
    "                                        max_depth=config.max_depth)\n",
    "\n",
    "    # model_on_test_acc, _ = fit_evaluate_model(X_tr_m=X_model, X_v_te=X_test, y_tr_m=y_model, y_v_te=y_test, model=model)\n",
    "    # train_on_test_acc, _ = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_test, y_tr_m=y_train, y_v_te=y_test, model=model)\n",
    "    train_on_val_acc, _ = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_val, y_tr_m=y_train, y_v_te=y_val, model=model)\n",
    "\n",
    "    wandb.log({\"accuracy\": train_on_val_acc})\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, train, count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\", # try grid or random\n",
    "    \"metric\": {\n",
    "      \"name\": \"accuracy\",\n",
    "      \"goal\": \"maximize\"   \n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"n_estimators\": {\n",
    "            \"values\": [1800, 2000, 2400, 2800]\n",
    "        },\n",
    "        \"max_depth\": {\n",
    "            \"values\": [20, 30, 40, 50]\n",
    "            # \"distribution\": \"uniform\",\n",
    "            # \"min\": 0.00001,\n",
    "            # \"max\": 1\n",
    "        },\n",
    "        \"min_samples_split\": {\n",
    "            \"values\": [2,5,10]\n",
    "        },\n",
    "        \"min_samples_leaf\": {\n",
    "            \"values\": [1,2,4]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project='02582_case1_rf', entity='tgml')\n",
    "\n",
    "def train():\n",
    "    config_defaults = {'bootstrap': True,\n",
    "    'criterion': 'mse',\n",
    "    'max_depth': None,\n",
    "    'max_features': 'auto',\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'min_impurity_split': None,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'n_estimators': 10,\n",
    "    'n_jobs': 1,\n",
    "    'oob_score': False,\n",
    "    'random_state': 42,\n",
    "    'verbose': 0,\n",
    "    'warm_start': False,\n",
    "    \"seed\": 0,\n",
    "    \"shuffle\": True}\n",
    "\n",
    "    wandb.init(project='02582_case1_1', entity='tgml', config=config_defaults)  # defaults are over-ridden during the sweep\n",
    "    config = wandb.config\n",
    "\n",
    "\n",
    "    # fit model on train\n",
    "    model = RandomForestRegressor(n_estimators=config.n_estimators, max_features=config.max_features,\n",
    "                            max_depth=config.max_depth, min_samples_split=config.min_samples_split, min_samples_leaf=config.min_samples_leaf)\n",
    "\n",
    "    # model_on_test_acc, _ = fit_evaluate_model(X_tr_m=X_model, X_v_te=X_test, y_tr_m=y_model, y_v_te=y_test, model=model)\n",
    "    # train_on_test_acc, _ = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_test, y_tr_m=y_train, y_v_te=y_test, model=model)\n",
    "    train_on_val_acc, _ = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_val, y_tr_m=y_train, y_v_te=y_val, model=model)\n",
    "\n",
    "    wandb.log({\"accuracy\": train_on_val_acc})\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, train, count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now make accuracy prediction using test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "M = 1000\n",
    "config = {\n",
    "    'n_estimators': 10,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    }\n",
    "\n",
    "np.random.seed(0)\n",
    "forecast_acc = []\n",
    "\n",
    "wandb.init(project='02582_case1_final', entity='tgml', config=config, group='SKLEARN_GB')\n",
    "\n",
    "for m in range(M):\n",
    "\n",
    "    ## Sample from test set\n",
    "    seed = np.random.randint(10000)\n",
    "    real_test_sampled = real_test\n",
    "    real_test_sampled = real_test_sampled.sample(n=len(real_test), replace=True, random_state=seed)\n",
    "    y_test = real_test_sampled.LoadFactor\n",
    "    X_test = real_test_sampled.loc[:, real_test.columns != 'LoadFactor']\n",
    "\n",
    "\n",
    "    ## Train and evaluate model\n",
    "    model = GradientBoostingRegressor(n_estimators=config['n_estimators'], \n",
    "                                    max_features=config['max_features'],\n",
    "                                    max_depth=config['max_depth'], \n",
    "                                    min_samples_split=config['min_samples_split'], \n",
    "                                    min_samples_leaf=config['min_samples_leaf'])\n",
    "\n",
    "    ## Evaluate best model on test data\n",
    "    test_acc, _ = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_test, y_tr_m=y_train, y_v_te=y_test, model=model)\n",
    "    forecast_acc.append(test_acc)\n",
    "    wandb.log({\"accuracy\": test_acc})\n",
    "\n",
    "\n",
    "print(f'Mean of test accuracies: {np.mean(forecast_acc)}\\nStd. of test accuracies: {np.var(forecast_acc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True\n",
    "M = 1000\n",
    "config = {\n",
    "    # 'bootstrap': True,\n",
    "    # 'criterion': 'mse',\n",
    "    # 'max_leaf_nodes': None,\n",
    "    # 'min_impurity_decrease': 0.0,\n",
    "    # 'min_impurity_split': None,\n",
    "    # 'min_weight_fraction_leaf': 0.0,\n",
    "    # 'n_jobs': 1,\n",
    "    # 'oob_score': False,\n",
    "    # 'random_state': 42,\n",
    "    # 'verbose': 0,\n",
    "    # 'warm_start': False,\n",
    "    # \"shuffle\": True,\n",
    "    'max_features': 'auto',\n",
    "    'n_estimators': 2000,\n",
    "    'max_depth': 30,\n",
    "    'min_samples_leaf': 4,\n",
    "    'min_samples_split': 4,\n",
    "    }\n",
    "\n",
    "np.random.seed(0)\n",
    "forecast_acc = []\n",
    "\n",
    "wandb.init(project='02582_case1_final', entity='tgml', config=config, group='RF')\n",
    "\n",
    "for m in range(M):\n",
    "\n",
    "    ## Sample from test set\n",
    "    seed = np.random.randint(10000)\n",
    "    real_test_sampled = real_test\n",
    "    real_test_sampled = real_test_sampled.sample(n=len(real_test), replace=True, random_state=seed)\n",
    "    y_test = real_test_sampled.LoadFactor\n",
    "    X_test = real_test_sampled.loc[:, real_test.columns != 'LoadFactor']\n",
    "\n",
    "\n",
    "    ## Train and evaluate model\n",
    "    model = RandomForestRegressor(n_estimators=config['n_estimators'], \n",
    "                                    max_features=config['max_features'],\n",
    "                                    max_depth=config['max_depth'], \n",
    "                                    min_samples_split=config['min_samples_split'], \n",
    "                                    min_samples_leaf=config['min_samples_leaf'])\n",
    "\n",
    "    ## Evaluate best model on test data\n",
    "    test_acc, _ = fit_evaluate_model(X_tr_m=X_train, X_v_te=X_test, y_tr_m=y_train, y_v_te=y_test, model=model)\n",
    "    forecast_acc.append(test_acc)\n",
    "    wandb.log({\"accuracy\": test_acc})\n",
    "\n",
    "\n",
    "print(f'Mean of test accuracies: {np.mean(forecast_acc)}\\nStd. of test accuracies: {np.var(forecast_acc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now have the best model, make predictions and save to output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realized = pd.read_csv('data/realized_preprocessed_data.csv', index_col=0)\n",
    "future = pd.read_csv('data/future_preprocessed_data.csv', index_col=0)\n",
    "\n",
    "# Remove target from data\n",
    "y = realized.LoadFactor\n",
    "realized = realized.loc[:, realized.columns != 'LoadFactor']\n",
    "\n",
    "X = realized\n",
    "\n",
    "# Fit model\n",
    "X_tr_m, fitted_scaler = normalize_seatcapacity_fit(X_train=X)\n",
    "\n",
    "## Fit model to the training data\n",
    "model.fit(X=X y=y)\n",
    "\n",
    "## Normalize validation data SeatCapacity for predictions\n",
    "X_future = normalize_seatcapacity(X_val=future, scaler=fitted_scaler)\n",
    "## Make predictions\n",
    "pred = model.predict(X_future)\n",
    "\n",
    "prediction_file = future\n",
    "prediction_file['LoadFactor'] = pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_file.to_txt('output.txt', sep=',', decimal='.')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
